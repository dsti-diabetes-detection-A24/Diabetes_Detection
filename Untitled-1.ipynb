{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "Provide an overview of the project, including the goal of predicting diabetic outcomes using the dataset and the steps involved in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Overview\n",
    "\n",
    "## Goal\n",
    "The primary goal of this project is to build a machine learning model to predict diabetic outcomes using the provided dataset. The model will be evaluated and compared with the PIMA dataset to ensure the coherence and reliability of our dataset.\n",
    "\n",
    "## Steps Involved\n",
    "1. **Import Required Libraries**: Import necessary libraries such as pandas, numpy, matplotlib, seaborn, and scikit-learn.\n",
    "2. **Load the Dataset**: Load the dataset into a pandas DataFrame for analysis.\n",
    "3. **Data Preprocessing**: Handle missing values, detect and handle outliers, and perform feature scaling.\n",
    "4. **Exploratory Data Analysis (EDA)**: Perform EDA to understand the distribution and relationships of the features.\n",
    "5. **Comparison with PIMA Dataset**: Compare the dataset with the PIMA dataset to verify the coherence of our dataset.\n",
    "6. **Modeling**: Split the data into training and testing sets, train a machine learning model, and evaluate its performance.\n",
    "7. **Feature Engineering**: Create new features to improve the model's performance.\n",
    "8. **Model Evaluation**: Evaluate the model using various metrics and interpret the results.\n",
    "9. **Improvements**: Look for potential improvements in the model and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Import\n",
    "Explain the process of collecting and importing the dataset, including the use of pandas to read the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection and Import\n",
    "\n",
    "## Explanation\n",
    "In this section, we will explain the process of collecting and importing the dataset. We will use the pandas library to read the CSV file containing the dataset. The dataset provides attributes for 15000 women on 8 features, and the variable to predict is the diabetic outcome.\n",
    "\n",
    "## Code\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "diabetes_csv = pd.read_csv('data/TAIPEI_diabetes.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to verify the import\n",
    "diabetes_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Discuss the steps taken to explore the data, including checking for missing values, visualizing distributions, and identifying potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Discuss the steps taken to explore the data, including checking for missing values, visualizing distributions, and identifying potential outliers.\n",
    "\n",
    "### Checking for Missing Values\n",
    "# Check for missing values in the dataset\n",
    "missing_values = diabetes_csv.isna().sum()\n",
    "missing_values\n",
    "\n",
    "### Visualizing Distributions\n",
    "# Visualize the distribution of each feature using histograms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "diabetes_csv.hist(figsize=(20, 10), bins=50, xlabelsize=8, ylabelsize=8)\n",
    "plt.show()\n",
    "\n",
    "### Identifying Potential Outliers\n",
    "# Use box plots to identify potential outliers in the numerical features\n",
    "numeric_cols = diabetes_csv.select_dtypes(include='number').columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=diabetes_csv[col])\n",
    "    plt.title(f\"Boxplot for {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the IQR to identify outliers\n",
    "Q1 = diabetes_csv.quantile(0.25)\n",
    "Q3 = diabetes_csv.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((diabetes_csv < (Q1 - 1.5 * IQR)) | (diabetes_csv > (Q3 + 1.5 * IQR))).sum()\n",
    "outliers\n",
    "\n",
    "# Calculate Z-scores to identify outliers\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs(stats.zscore(diabetes_csv.select_dtypes(include=[np.number])))\n",
    "outliers_z = (z_scores > 3).sum(axis=0)\n",
    "outliers_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values and Outliers\n",
    "Describe the methods used to handle missing values and outliers, including the use of IQR and Z-score methods to detect outliers and the decision to remove or cap outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = diabetes_csv.isna().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Explain the feature engineering steps taken, such as creating age groups and BMI categories, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Explanation\n",
    "In this section, we will explain the feature engineering steps taken to improve the model's performance. This includes creating age groups and BMI categories, and encoding categorical variables.\n",
    "\n",
    "## Code\n",
    "\n",
    "# Create age groups\n",
    "diabetes_csv['AgeGroup'] = pd.cut(diabetes_csv['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '31-40', '41-50', '51-60', '61-70', '71-80'])\n",
    "\n",
    "# Create BMI categories\n",
    "diabetes_csv['BMICategory'] = pd.cut(diabetes_csv['BMI'], bins=[0, 18.5, 24.9, 29.9, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "# One-hot encode the categorical variables\n",
    "diabetes_csv = pd.get_dummies(diabetes_csv, columns=['AgeGroup', 'BMICategory'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the dataset to verify the changes\n",
    "diabetes_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Detail the modeling process, including the choice of logistic regression, data preprocessing, splitting the data into training and testing sets, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "## Explanation\n",
    "In this section, we will detail the modeling process, including the choice of logistic regression, data preprocessing, splitting the data into training and testing sets, and training the model.\n",
    "\n",
    "### Choice of Logistic Regression\n",
    "Logistic regression is chosen for this project because it is a simple yet effective algorithm for binary classification problems. It provides probabilities for class membership and is easy to interpret.\n",
    "\n",
    "### Data Preprocessing\n",
    "We will preprocess the data by scaling the features to ensure that they are on the same scale. This helps improve the performance of the logistic regression model.\n",
    "\n",
    "### Splitting the Data\n",
    "We will split the data into training and testing sets to evaluate the model's performance on unseen data.\n",
    "\n",
    "### Training the Model\n",
    "We will train a logistic regression model using the training data and evaluate its performance using the testing data.\n",
    "\n",
    "## Code\n",
    "\n",
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Drop unnecessary columns\n",
    "diabetes_csv = diabetes_csv.drop(columns=['PatientID'], errors='ignore')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = diabetes_csv.drop(columns=['Diabetic'])\n",
    "y = diabetes_csv['Diabetic']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Comparison with PIMA Dataset\n",
    "Discuss the evaluation metrics used to assess the model's performance, including accuracy, precision, recall, and F1 score. Explain the comparison with the PIMA dataset and how it was used as a reference to verify the coherence of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Comparison with PIMA Dataset\n",
    "\n",
    "## Discuss the evaluation metrics used to assess the model's performance, including accuracy, precision, recall, and F1 score.\n",
    "\n",
    "### Evaluation Metrics\n",
    "In this section, we will discuss the evaluation metrics used to assess the performance of our logistic regression model. The metrics include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "- **Accuracy**: The proportion of correctly classified instances among the total instances.\n",
    "- **Precision**: The proportion of true positive instances among the instances predicted as positive.\n",
    "- **Recall**: The proportion of true positive instances among the actual positive instances.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "### Model Evaluation Results\n",
    "The evaluation results for our logistic regression model are as follows:\n",
    "\n",
    "- **Accuracy**: {accuracy}\n",
    "- **Confusion Matrix**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Summarize the findings and results of the project, including the performance of the model and any insights gained from the comparison with the PIMA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "\n",
    "## Summary of Findings\n",
    "In this project, we aimed to build a machine learning model to predict diabetic outcomes using the provided dataset. We followed a structured approach, including data preprocessing, exploratory data analysis, feature engineering, modeling, and evaluation. The key findings and results are summarized below:\n",
    "\n",
    "### Data Preprocessing and EDA\n",
    "- We handled missing values and identified potential outliers using both the IQR and Z-score methods.\n",
    "- Visualizations such as histograms and box plots helped us understand the distribution of features and detect outliers.\n",
    "\n",
    "### Feature Engineering\n",
    "- We created new features such as age groups and BMI categories to improve the model's performance.\n",
    "- One-hot encoding was applied to convert categorical variables into numerical format.\n",
    "\n",
    "### Modeling\n",
    "- We chose logistic regression for its simplicity and effectiveness in binary classification problems.\n",
    "- The data was split into training and testing sets, and features were scaled to ensure uniformity.\n",
    "- The logistic regression model was trained and evaluated using various metrics.\n",
    "\n",
    "### Model Evaluation\n",
    "- The model achieved an accuracy of {accuracy}, with the following confusion matrix and classification report:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
